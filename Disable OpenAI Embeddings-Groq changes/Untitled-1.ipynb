{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path='/Users/madmax_jos/Documents/agentaicourse/.env')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apkey=os.getenv(\"GROQ_API_KEY\")\n",
    "if apkey is None:\n",
    "    raise ValueError(\"GROQ_API_KEY environment variable is not set or not loaded correctly.\")\n",
    "print(f\"GROQ_API_KEY: {apkey}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()  # This will load .env from the root directory by default\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY loaded successfully.\")\n",
    "\n",
    "# Now proceed with your original logic (the rest of your code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Union, Dict, Any\n",
    "from hashlib import md5\n",
    "\n",
    "try:\n",
    "    from sqlalchemy.dialects import postgresql\n",
    "    from sqlalchemy.engine import create_engine, Engine\n",
    "    from sqlalchemy.inspection import inspect\n",
    "    from sqlalchemy.orm import Session, sessionmaker\n",
    "    from sqlalchemy.schema import MetaData, Table, Column\n",
    "    from sqlalchemy.sql.expression import text, func, select\n",
    "    from sqlalchemy.types import DateTime, String\n",
    "except ImportError:\n",
    "    raise ImportError(\"`sqlalchemy` not installed\")\n",
    "\n",
    "try:\n",
    "    from pgvector.sqlalchemy import Vector\n",
    "except ImportError:\n",
    "    raise ImportError(\"`pgvector` not installed\")\n",
    "\n",
    "from phi.document import Document\n",
    "from phi.embedder import Embedder\n",
    "from phi.vectordb.base import VectorDb\n",
    "from phi.vectordb.distance import Distance\n",
    "from phi.vectordb.pgvector.index import Ivfflat, HNSW\n",
    "from phi.utils.log import logger\n",
    "from phi.reranker.base import Reranker\n",
    "\n",
    "\n",
    "class PgVector2(VectorDb):\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection: str,\n",
    "        schema: Optional[str] = \"ai\",\n",
    "        db_url: Optional[str] = None,\n",
    "        db_engine: Optional[Engine] = None,\n",
    "        embedder: Optional[Embedder] = None,\n",
    "        distance: Distance = Distance.cosine,\n",
    "        index: Optional[Union[Ivfflat, HNSW]] = HNSW(),\n",
    "        reranker: Optional[Reranker] = None,\n",
    "    ):\n",
    "        _engine: Optional[Engine] = db_engine\n",
    "        if _engine is None and db_url is not None:\n",
    "            _engine = create_engine(db_url)\n",
    "\n",
    "        if _engine is None:\n",
    "            raise ValueError(\"Must provide either db_url or db_engine\")\n",
    "\n",
    "        # Collection attributes\n",
    "        self.collection: str = collection\n",
    "        self.schema: Optional[str] = schema\n",
    "\n",
    "        # Database attributes\n",
    "        self.db_url: Optional[str] = db_url\n",
    "        self.db_engine: Engine = _engine\n",
    "        self.metadata: MetaData = MetaData(schema=self.schema)\n",
    "\n",
    "        # Embedder for embedding the document contents\n",
    "        _embedder = embedder\n",
    "        # if _embedder is None:\n",
    "        #     from phi.embedder.openai import OpenAIEmbedder\n",
    "\n",
    "        #     _embedder = OpenAIEmbedder()\n",
    "          if _embedder is None:\n",
    "              from groq_embedder import GroqEmbedder  # Import your custom GroqEmbedder\n",
    "              _embedder = GroqEmbedder(api_key=\"your_groq_api_key\")  # Initialize GroqEmbedder with your API key\n",
    "\n",
    "        self.embedder: Embedder = _embedder\n",
    "        self.dimensions: Optional[int] = self.embedder.dimensions\n",
    "\n",
    "        # Distance metric\n",
    "        self.distance: Distance = distance\n",
    "\n",
    "        # Reranker instance\n",
    "        self.reranker: Optional[Reranker] = reranker\n",
    "\n",
    "        # Index for the collection\n",
    "        self.index: Optional[Union[Ivfflat, HNSW]] = index\n",
    "\n",
    "        # Database session\n",
    "        self.Session: sessionmaker[Session] = sessionmaker(bind=self.db_engine)\n",
    "\n",
    "        # Database table for the collection\n",
    "        self.table: Table = self.get_table()\n",
    "\n",
    "    def get_table(self) -> Table:\n",
    "        return Table(\n",
    "            self.collection,\n",
    "            self.metadata,\n",
    "            Column(\"id\", String, primary_key=True),\n",
    "            Column(\"name\", String),\n",
    "            Column(\"meta_data\", postgresql.JSONB, server_default=text(\"'{}'::jsonb\")),\n",
    "            Column(\"content\", postgresql.TEXT),\n",
    "            Column(\"embedding\", Vector(self.dimensions)),\n",
    "            Column(\"usage\", postgresql.JSONB),\n",
    "            Column(\"created_at\", DateTime(timezone=True), server_default=text(\"now()\")),\n",
    "            Column(\"updated_at\", DateTime(timezone=True), onupdate=text(\"now()\")),\n",
    "            Column(\"content_hash\", String),\n",
    "            extend_existing=True,\n",
    "        )\n",
    "\n",
    "    def table_exists(self) -> bool:\n",
    "        logger.debug(f\"Checking if table exists: {self.table.name}\")\n",
    "        try:\n",
    "            return inspect(self.db_engine).has_table(self.table.name, schema=self.schema)\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            return False\n",
    "\n",
    "    def create(self) -> None:\n",
    "        if not self.table_exists():\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(\"Creating extension: vector\")\n",
    "                    sess.execute(text(\"create extension if not exists vector;\"))\n",
    "                    if self.schema is not None:\n",
    "                        logger.debug(f\"Creating schema: {self.schema}\")\n",
    "                        sess.execute(text(f\"create schema if not exists {self.schema};\"))\n",
    "            logger.debug(f\"Creating table: {self.collection}\")\n",
    "            self.table.create(self.db_engine)\n",
    "\n",
    "    def doc_exists(self, document: Document) -> bool:\n",
    "        \"\"\"\n",
    "        Validating if the document exists or not\n",
    "\n",
    "        Args:\n",
    "            document (Document): Document to validate\n",
    "        \"\"\"\n",
    "        columns = [self.table.c.name, self.table.c.content_hash]\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                stmt = select(*columns).where(self.table.c.content_hash == md5(cleaned_content.encode()).hexdigest())\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def name_exists(self, name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if a row with this name exists or not\n",
    "\n",
    "        Args:\n",
    "            name (str): Name to check\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(self.table.c.name).where(self.table.c.name == name)\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def id_exists(self, id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if a row with this id exists or not\n",
    "\n",
    "        Args:\n",
    "            id (str): Id to check\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(self.table.c.id).where(self.table.c.id == id)\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def insert(self, documents: List[Document], filters: Optional[Dict[str, Any]] = None, batch_size: int = 10) -> None:\n",
    "        with self.Session() as sess:\n",
    "            counter = 0\n",
    "            for document in documents:\n",
    "                document.embed(embedder=self.embedder)\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                content_hash = md5(cleaned_content.encode()).hexdigest()\n",
    "                _id = document.id or content_hash\n",
    "                stmt = postgresql.insert(self.table).values(\n",
    "                    id=_id,\n",
    "                    name=document.name,\n",
    "                    meta_data=document.meta_data,\n",
    "                    content=cleaned_content,\n",
    "                    embedding=document.embedding,\n",
    "                    usage=document.usage,\n",
    "                    content_hash=content_hash,\n",
    "                )\n",
    "                sess.execute(stmt)\n",
    "                counter += 1\n",
    "                logger.debug(f\"Inserted document: {document.name} ({document.meta_data})\")\n",
    "\n",
    "                # Commit every `batch_size` documents\n",
    "                if counter >= batch_size:\n",
    "                    sess.commit()\n",
    "                    logger.info(f\"Committed {counter} documents\")\n",
    "                    counter = 0\n",
    "\n",
    "            # Commit any remaining documents\n",
    "            if counter > 0:\n",
    "                sess.commit()\n",
    "                logger.info(f\"Committed {counter} documents\")\n",
    "\n",
    "    def upsert_available(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def upsert(self, documents: List[Document], filters: Optional[Dict[str, Any]] = None, batch_size: int = 20) -> None:\n",
    "        \"\"\"\n",
    "        Upsert documents into the database.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of documents to upsert\n",
    "            filters (Optional[Dict[str, Any]]): Filters to apply while upserting documents\n",
    "            batch_size (int): Batch size for upserting documents\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            counter = 0\n",
    "            for document in documents:\n",
    "                document.embed(embedder=self.embedder)\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                content_hash = md5(cleaned_content.encode()).hexdigest()\n",
    "                _id = document.id or content_hash\n",
    "                stmt = postgresql.insert(self.table).values(\n",
    "                    id=_id,\n",
    "                    name=document.name,\n",
    "                    meta_data=document.meta_data,\n",
    "                    content=cleaned_content,\n",
    "                    embedding=document.embedding,\n",
    "                    usage=document.usage,\n",
    "                    content_hash=content_hash,\n",
    "                )\n",
    "                # Update row when id matches but 'content_hash' is different\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=[\"id\"],\n",
    "                    set_=dict(\n",
    "                        name=stmt.excluded.name,\n",
    "                        meta_data=stmt.excluded.meta_data,\n",
    "                        content=stmt.excluded.content,\n",
    "                        embedding=stmt.excluded.embedding,\n",
    "                        usage=stmt.excluded.usage,\n",
    "                        content_hash=stmt.excluded.content_hash,\n",
    "                        updated_at=text(\"now()\"),\n",
    "                    ),\n",
    "                )\n",
    "                sess.execute(stmt)\n",
    "                counter += 1\n",
    "                logger.debug(f\"Upserted document: {document.id} | {document.name} | {document.meta_data}\")\n",
    "\n",
    "                # Commit every `batch_size` documents\n",
    "                if counter >= batch_size:\n",
    "                    sess.commit()\n",
    "                    logger.info(f\"Committed {counter} documents\")\n",
    "                    counter = 0\n",
    "\n",
    "            # Commit any remaining documents\n",
    "            if counter > 0:\n",
    "                sess.commit()\n",
    "                logger.info(f\"Committed {counter} documents\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Document]:\n",
    "        query_embedding = self.embedder.get_embedding(query)\n",
    "        if query_embedding is None:\n",
    "            logger.error(f\"Error getting embedding for Query: {query}\")\n",
    "            return []\n",
    "\n",
    "        columns = [\n",
    "            self.table.c.name,\n",
    "            self.table.c.meta_data,\n",
    "            self.table.c.content,\n",
    "            self.table.c.embedding,\n",
    "            self.table.c.usage,\n",
    "        ]\n",
    "\n",
    "        stmt = select(*columns)\n",
    "\n",
    "        if filters is not None:\n",
    "            for key, value in filters.items():\n",
    "                if hasattr(self.table.c, key):\n",
    "                    stmt = stmt.where(getattr(self.table.c, key) == value)\n",
    "\n",
    "        if self.distance == Distance.l2:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.max_inner_product(query_embedding))\n",
    "        if self.distance == Distance.cosine:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.cosine_distance(query_embedding))\n",
    "        if self.distance == Distance.max_inner_product:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.max_inner_product(query_embedding))\n",
    "\n",
    "        stmt = stmt.limit(limit=limit)\n",
    "        logger.debug(f\"Query: {stmt}\")\n",
    "\n",
    "        # Get neighbors\n",
    "        try:\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    if self.index is not None:\n",
    "                        if isinstance(self.index, Ivfflat):\n",
    "                            sess.execute(text(f\"SET LOCAL ivfflat.probes = {self.index.probes}\"))\n",
    "                        elif isinstance(self.index, HNSW):\n",
    "                            sess.execute(text(f\"SET LOCAL hnsw.ef_search  = {self.index.ef_search}\"))\n",
    "                    neighbors = sess.execute(stmt).fetchall() or []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching for documents: {e}\")\n",
    "            logger.error(\"Table might not exist, creating for future use\")\n",
    "            self.create()\n",
    "            return []\n",
    "\n",
    "        # Build search results\n",
    "        search_results: List[Document] = []\n",
    "        for neighbor in neighbors:\n",
    "            search_results.append(\n",
    "                Document(\n",
    "                    name=neighbor.name,\n",
    "                    meta_data=neighbor.meta_data,\n",
    "                    content=neighbor.content,\n",
    "                    embedder=self.embedder,\n",
    "                    embedding=neighbor.embedding,\n",
    "                    usage=neighbor.usage,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if self.reranker:\n",
    "            search_results = self.reranker.rerank(query=query, documents=search_results)\n",
    "\n",
    "        return search_results\n",
    "\n",
    "    def drop(self) -> None:\n",
    "        if self.table_exists():\n",
    "            logger.debug(f\"Deleting table: {self.collection}\")\n",
    "            self.table.drop(self.db_engine)\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        return self.table_exists()\n",
    "\n",
    "    def get_count(self) -> int:\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(func.count(self.table.c.name)).select_from(self.table)\n",
    "                result = sess.execute(stmt).scalar()\n",
    "                if result is not None:\n",
    "                    return int(result)\n",
    "                return 0\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        from math import sqrt\n",
    "\n",
    "        logger.debug(\"==== Optimizing Vector DB ====\")\n",
    "        if self.index is None:\n",
    "            return\n",
    "\n",
    "        if self.index.name is None:\n",
    "            _type = \"ivfflat\" if isinstance(self.index, Ivfflat) else \"hnsw\"\n",
    "            self.index.name = f\"{self.collection}_{_type}_index\"\n",
    "\n",
    "        index_distance = \"vector_cosine_ops\"\n",
    "        if self.distance == Distance.l2:\n",
    "            index_distance = \"vector_l2_ops\"\n",
    "        if self.distance == Distance.max_inner_product:\n",
    "            index_distance = \"vector_ip_ops\"\n",
    "\n",
    "        if isinstance(self.index, Ivfflat):\n",
    "            num_lists = self.index.lists\n",
    "            if self.index.dynamic_lists:\n",
    "                total_records = self.get_count()\n",
    "                logger.debug(f\"Number of records: {total_records}\")\n",
    "                if total_records < 1000000:\n",
    "                    num_lists = int(total_records / 1000)\n",
    "                elif total_records > 1000000:\n",
    "                    num_lists = int(sqrt(total_records))\n",
    "\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(f\"Setting configuration: {self.index.configuration}\")\n",
    "                    for key, value in self.index.configuration.items():\n",
    "                        sess.execute(text(f\"SET {key} = '{value}';\"))\n",
    "                    logger.debug(\n",
    "                        f\"Creating Ivfflat index with lists: {num_lists}, probes: {self.index.probes} \"\n",
    "                        f\"and distance metric: {index_distance}\"\n",
    "                    )\n",
    "                    sess.execute(text(f\"SET ivfflat.probes = {self.index.probes};\"))\n",
    "                    sess.execute(\n",
    "                        text(\n",
    "                            f\"CREATE INDEX IF NOT EXISTS {self.index.name} ON {self.table} \"\n",
    "                            f\"USING ivfflat (embedding {index_distance}) \"\n",
    "                            f\"WITH (lists = {num_lists});\"\n",
    "                        )\n",
    "                    )\n",
    "        elif isinstance(self.index, HNSW):\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(f\"Setting configuration: {self.index.configuration}\")\n",
    "                    for key, value in self.index.configuration.items():\n",
    "                        sess.execute(text(f\"SET {key} = '{value}';\"))\n",
    "                    logger.debug(\n",
    "                        f\"Creating HNSW index with m: {self.index.m}, ef_construction: {self.index.ef_construction} \"\n",
    "                        f\"and distance metric: {index_distance}\"\n",
    "                    )\n",
    "                    sess.execute(\n",
    "                        text(\n",
    "                            f\"CREATE INDEX IF NOT EXISTS {self.index.name} ON {self.table} \"\n",
    "                            f\"USING hnsw (embedding {index_distance}) \"\n",
    "                            f\"WITH (m = {self.index.m}, ef_construction = {self.index.ef_construction});\"\n",
    "                        )\n",
    "                    )\n",
    "        logger.debug(\"==== Optimized Vector DB ====\")\n",
    "\n",
    "    def delete(self) -> bool:\n",
    "        from sqlalchemy import delete\n",
    "\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = delete(self.table)\n",
    "                sess.execute(stmt)\n",
    "                return True\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"\n",
    "        Create a deep copy of the PgVector instance, handling unpickleable attributes.\n",
    "\n",
    "        Args:\n",
    "            memo (dict): A dictionary of objects already copied during the current copying pass.\n",
    "\n",
    "        Returns:\n",
    "            PgVector: A deep-copied instance of PgVector.\n",
    "        \"\"\"\n",
    "        from copy import deepcopy\n",
    "\n",
    "        # Create a new instance without calling __init__\n",
    "        cls = self.__class__\n",
    "        copied_obj = cls.__new__(cls)\n",
    "        memo[id(self)] = copied_obj\n",
    "\n",
    "        # Deep copy attributes\n",
    "        for k, v in self.__dict__.items():\n",
    "            if k in {\"metadata\", \"table\"}:\n",
    "                continue\n",
    "            # Reuse db_engine and Session without copying\n",
    "            elif k in {\"db_engine\", \"Session\", \"embedder\"}:\n",
    "                setattr(copied_obj, k, v)\n",
    "            else:\n",
    "                setattr(copied_obj, k, deepcopy(v, memo))\n",
    "\n",
    "        # Recreate metadata and table for the copied instance\n",
    "        copied_obj.metadata = MetaData(schema=copied_obj.schema)\n",
    "        copied_obj.table = copied_obj.get_table()\n",
    "\n",
    "        return copied_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Union, Dict, Any\n",
    "from hashlib import md5\n",
    "\n",
    "try:\n",
    "    from sqlalchemy.dialects import postgresql\n",
    "    from sqlalchemy.engine import create_engine, Engine\n",
    "    from sqlalchemy.inspection import inspect\n",
    "    from sqlalchemy.orm import Session, sessionmaker\n",
    "    from sqlalchemy.schema import MetaData, Table, Column\n",
    "    from sqlalchemy.sql.expression import text, func, select\n",
    "    from sqlalchemy.types import DateTime, String\n",
    "except ImportError:\n",
    "    raise ImportError(\"`sqlalchemy` not installed\")\n",
    "\n",
    "try:\n",
    "    from pgvector.sqlalchemy import Vector\n",
    "except ImportError:\n",
    "    raise ImportError(\"`pgvector` not installed\")\n",
    "\n",
    "from phi.document import Document\n",
    "from phi.embedder import Embedder\n",
    "from phi.vectordb.base import VectorDb\n",
    "from phi.vectordb.distance import Distance\n",
    "from phi.vectordb.pgvector.index import Ivfflat, HNSW\n",
    "from phi.utils.log import logger\n",
    "from phi.reranker.base import Reranker\n",
    "\n",
    "class PgVector2(VectorDb):\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection: str,\n",
    "        schema: Optional[str] = \"ai\",\n",
    "        db_url: Optional[str] = None,\n",
    "        db_engine: Optional[Engine] = None,\n",
    "        embedder: Optional[Embedder] = None,\n",
    "        distance: Distance = Distance.cosine,\n",
    "        index: Optional[Union[Ivfflat, HNSW]] = HNSW(),\n",
    "        reranker: Optional[Reranker] = None,\n",
    "    ):\n",
    "        _engine: Optional[Engine] = db_engine\n",
    "        if _engine is None and db_url is not None:\n",
    "            _engine = create_engine(db_url)\n",
    "\n",
    "        if _engine is None:\n",
    "            raise ValueError(\"Must provide either db_url or db_engine\")\n",
    "\n",
    "        # Collection attributes\n",
    "        self.collection: str = collection\n",
    "        self.schema: Optional[str] = schema\n",
    "\n",
    "        # Database attributes\n",
    "        self.db_url: Optional[str] = db_url\n",
    "        self.db_engine: Engine = _engine\n",
    "        self.metadata: MetaData = MetaData(schema=self.schema)\n",
    "\n",
    "        # Embedder for embedding the document contents\n",
    "        _embedder = embedder\n",
    "        if _embedder is None:\n",
    "            # Import GroqEmbedder instead of OpenAIEmbedder\n",
    "            from groq_embedder import GroqEmbedder  # Import your custom GroqEmbedder\n",
    "            _embedder = GroqEmbedder(api_key=\"your_groq_api_key\")  # Initialize GroqEmbedder with your API key\n",
    "\n",
    "        self.embedder: Embedder = _embedder\n",
    "        self.dimensions: Optional[int] = self.embedder.dimensions\n",
    "\n",
    "        # Distance metric\n",
    "        self.distance: Distance = distance\n",
    "\n",
    "        # Reranker instance\n",
    "        self.reranker: Optional[Reranker] = reranker\n",
    "\n",
    "        # Index for the collection\n",
    "        self.index: Optional[Union[Ivfflat, HNSW]] = index\n",
    "\n",
    "        # Database session\n",
    "        self.Session: sessionmaker[Session] = sessionmaker(bind=self.db_engine)\n",
    "\n",
    "        # Database table for the collection\n",
    "        self.table: Table = self.get_table()\n",
    "\n",
    "    def get_table(self) -> Table:\n",
    "        return Table(\n",
    "            self.collection,\n",
    "            self.metadata,\n",
    "            Column(\"id\", String, primary_key=True),\n",
    "            Column(\"name\", String),\n",
    "            Column(\"meta_data\", postgresql.JSONB, server_default=text(\"'{}'::jsonb\")),\n",
    "            Column(\"content\", postgresql.TEXT),\n",
    "            Column(\"embedding\", Vector(self.dimensions)),\n",
    "            Column(\"usage\", postgresql.JSONB),\n",
    "            Column(\"created_at\", DateTime(timezone=True), server_default=text(\"now()\")),\n",
    "            Column(\"updated_at\", DateTime(timezone=True), onupdate=text(\"now()\")),\n",
    "            Column(\"content_hash\", String),\n",
    "            extend_existing=True,\n",
    "        )\n",
    "\n",
    "    def table_exists(self) -> bool:\n",
    "        logger.debug(f\"Checking if table exists: {self.table.name}\")\n",
    "        try:\n",
    "            return inspect(self.db_engine).has_table(self.table.name, schema=self.schema)\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            return False\n",
    "\n",
    "    def create(self) -> None:\n",
    "        if not self.table_exists():\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(\"Creating extension: vector\")\n",
    "                    sess.execute(text(\"create extension if not exists vector;\"))\n",
    "                    if self.schema is not None:\n",
    "                        logger.debug(f\"Creating schema: {self.schema}\")\n",
    "                        sess.execute(text(f\"create schema if not exists {self.schema};\"))\n",
    "            logger.debug(f\"Creating table: {self.collection}\")\n",
    "            self.table.create(self.db_engine)\n",
    "\n",
    "    def doc_exists(self, document: Document) -> bool:\n",
    "        \"\"\"\n",
    "        Validating if the document exists or not\n",
    "\n",
    "        Args:\n",
    "            document (Document): Document to validate\n",
    "        \"\"\"\n",
    "        columns = [self.table.c.name, self.table.c.content_hash]\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                stmt = select(*columns).where(self.table.c.content_hash == md5(cleaned_content.encode()).hexdigest())\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def name_exists(self, name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if a row with this name exists or not\n",
    "\n",
    "        Args:\n",
    "            name (str): Name to check\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(self.table.c.name).where(self.table.c.name == name)\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def id_exists(self, id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if a row with this id exists or not\n",
    "\n",
    "        Args:\n",
    "            id (str): Id to check\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(self.table.c.id).where(self.table.c.id == id)\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def insert(self, documents: List[Document], filters: Optional[Dict[str, Any]] = None, batch_size: int = 10) -> None:\n",
    "        with self.Session() as sess:\n",
    "            counter = 0\n",
    "            for document in documents:\n",
    "                document.embed(embedder=self.embedder)\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                content_hash = md5(cleaned_content.encode()).hexdigest()\n",
    "                _id = document.id or content_hash\n",
    "                stmt = postgresql.insert(self.table).values(\n",
    "                    id=_id,\n",
    "                    name=document.name,\n",
    "                    meta_data=document.meta_data,\n",
    "                    content=cleaned_content,\n",
    "                    embedding=document.embedding,\n",
    "                    usage=document.usage,\n",
    "                    content_hash=content_hash,\n",
    "                )\n",
    "                sess.execute(stmt)\n",
    "                counter += 1\n",
    "                logger.debug(f\"Inserted document: {document.name} ({document.meta_data})\")\n",
    "\n",
    "                # Commit every `batch_size` documents\n",
    "                if counter >= batch_size:\n",
    "                    sess.commit()\n",
    "                    logger.info(f\"Committed {counter} documents\")\n",
    "                    counter = 0\n",
    "\n",
    "            # Commit any remaining documents\n",
    "            if counter > 0:\n",
    "                sess.commit()\n",
    "                logger.info(f\"Committed {counter} documents\")\n",
    "\n",
    "    def upsert_available(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def upsert(self, documents: List[Document], filters: Optional[Dict[str, Any]] = None, batch_size: int = 20) -> None:\n",
    "        \"\"\"\n",
    "        Upsert documents into the database.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of documents to upsert\n",
    "            filters (Optional[Dict[str, Any]]): Filters to apply while upserting documents\n",
    "            batch_size (int): Batch size for upserting documents\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            counter = 0\n",
    "            for document in documents:\n",
    "                document.embed(embedder=self.embedder)\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                content_hash = md5(cleaned_content.encode()).hexdigest()\n",
    "                _id = document.id or content_hash\n",
    "                stmt = postgresql.insert(self.table).values(\n",
    "                    id=_id,\n",
    "                    name=document.name,\n",
    "                    meta_data=document.meta_data,\n",
    "                    content=cleaned_content,\n",
    "                    embedding=document.embedding,\n",
    "                    usage=document.usage,\n",
    "                    content_hash=content_hash,\n",
    "                )\n",
    "                # Update row when id matches but 'content_hash' is different\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=[\"id\"],\n",
    "                    set_=dict(\n",
    "                        name=stmt.excluded.name,\n",
    "                        meta_data=stmt.excluded.meta_data,\n",
    "                        content=stmt.excluded.content,\n",
    "                        embedding=stmt.excluded.embedding,\n",
    "                        usage=stmt.excluded.usage,\n",
    "                        content_hash=stmt.excluded.content_hash,\n",
    "                        updated_at=text(\"now()\"),\n",
    "                    ),\n",
    "                )\n",
    "                sess.execute(stmt)\n",
    "                counter += 1\n",
    "                logger.debug(f\"Upserted document: {document.id} | {document.name} | {document.meta_data}\")\n",
    "\n",
    "                # Commit every `batch_size` documents\n",
    "                if counter >= batch_size:\n",
    "                    sess.commit()\n",
    "                    logger.info(f\"Committed {counter} documents\")\n",
    "                    counter = 0\n",
    "\n",
    "            # Commit any remaining documents\n",
    "            if counter > 0:\n",
    "                sess.commit()\n",
    "                logger.info(f\"Committed {counter} documents\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Document]:\n",
    "        query_embedding = self.embedder.get_embedding(query)\n",
    "        if query_embedding is None:\n",
    "            logger.error(f\"Error getting embedding for Query: {query}\")\n",
    "            return []\n",
    "\n",
    "        columns = [\n",
    "            self.table.c.name,\n",
    "            self.table.c.meta_data,\n",
    "            self.table.c.content,\n",
    "            self.table.c.embedding,\n",
    "            self.table.c.usage,\n",
    "        ]\n",
    "\n",
    "        stmt = select(*columns)\n",
    "\n",
    "        if filters is not None:\n",
    "            for key, value in filters.items():\n",
    "                if hasattr(self.table.c, key):\n",
    "                    stmt = stmt.where(getattr(self.table.c, key) == value)\n",
    "\n",
    "        if self.distance == Distance.l2:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.max_inner_product(query_embedding))\n",
    "        if self.distance == Distance.cosine:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.cosine_distance(query_embedding))\n",
    "        if self.distance == Distance.max_inner_product:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.max_inner_product(query_embedding))\n",
    "\n",
    "        stmt = stmt.limit(limit=limit)\n",
    "        logger.debug(f\"Query: {stmt}\")\n",
    "\n",
    "        # Get neighbors\n",
    "        try:\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    if self.index is not None:\n",
    "                        if isinstance(self.index, Ivfflat):\n",
    "                            sess.execute(text(f\"SET LOCAL ivfflat.probes = {self.index.probes}\"))\n",
    "                        elif isinstance(self.index, HNSW):\n",
    "                            sess.execute(text(f\"SET LOCAL hnsw.ef_search  = {self.index.ef_search}\"))\n",
    "                    neighbors = sess.execute(stmt).fetchall() or []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching for documents: {e}\")\n",
    "            logger.error(\"Table might not exist, creating for future use\")\n",
    "            self.create()\n",
    "            return []\n",
    "\n",
    "        # Build search results\n",
    "        search_results: List[Document] = []\n",
    "        for neighbor in neighbors:\n",
    "            search_results.append(\n",
    "                Document(\n",
    "                    name=neighbor.name,\n",
    "                    meta_data=neighbor.meta_data,\n",
    "                    content=neighbor.content,\n",
    "                    embedder=self.embedder,\n",
    "                    embedding=neighbor.embedding,\n",
    "                    usage=neighbor.usage,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if self.reranker:\n",
    "            search_results = self.reranker.rerank(query=query, documents=search_results)\n",
    "\n",
    "        return search_results\n",
    "\n",
    "    def drop(self) -> None:\n",
    "        if self.table_exists():\n",
    "            logger.debug(f\"Deleting table: {self.collection}\")\n",
    "            self.table.drop(self.db_engine)\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        return self.table_exists()\n",
    "\n",
    "    def get_count(self) -> int:\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(func.count(self.table.c.name)).select_from(self.table)\n",
    "                result = sess.execute(stmt).scalar()\n",
    "                if result is not None:\n",
    "                    return int(result)\n",
    "                return 0\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        from math import sqrt\n",
    "\n",
    "        logger.debug(\"==== Optimizing Vector DB ====\")\n",
    "        if self.index is None:\n",
    "            return\n",
    "\n",
    "        if self.index.name is None:\n",
    "            _type = \"ivfflat\" if isinstance(self.index, Ivfflat) else \"hnsw\"\n",
    "            self.index.name = f\"{self.collection}_{_type}_index\"\n",
    "\n",
    "        index_distance = \"vector_cosine_ops\"\n",
    "        if self.distance == Distance.l2:\n",
    "            index_distance = \"vector_l2_ops\"\n",
    "        if self.distance == Distance.max_inner_product:\n",
    "            index_distance = \"vector_ip_ops\"\n",
    "\n",
    "        if isinstance(self.index, Ivfflat):\n",
    "            num_lists = self.index.lists\n",
    "            if self.index.dynamic_lists:\n",
    "                total_records = self.get_count()\n",
    "                logger.debug(f\"Number of records: {total_records}\")\n",
    "                if total_records < 1000000:\n",
    "                    num_lists = int(total_records / 1000)\n",
    "                elif total_records > 1000000:\n",
    "                    num_lists = int(sqrt(total_records))\n",
    "\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(f\"Setting configuration: {self.index.configuration}\")\n",
    "                    for key, value in self.index.configuration.items():\n",
    "                        sess.execute(text(f\"SET {key} = '{value}';\"))\n",
    "                    logger.debug(\n",
    "                        f\"Creating Ivfflat index with lists: {num_lists}, probes: {self.index.probes} \"\n",
    "                        f\"and distance metric: {index_distance}\"\n",
    "                    )\n",
    "                    sess.execute(text(f\"SET ivfflat.probes = {self.index.probes};\"))\n",
    "                    sess.execute(\n",
    "                        text(\n",
    "                            f\"CREATE INDEX IF NOT EXISTS {self.index.name} ON {self.table} \"\n",
    "                            f\"USING ivfflat (embedding {index_distance}) \"\n",
    "                            f\"WITH (lists = {num_lists});\"\n",
    "                        )\n",
    "                    )\n",
    "        elif isinstance(self.index, HNSW):\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(f\"Setting configuration: {self.index.configuration}\")\n",
    "                    for key, value in self.index.configuration.items():\n",
    "                        sess.execute(text(f\"SET {key} = '{value}';\"))\n",
    "                    logger.debug(\n",
    "                        f\"Creating HNSW index with m: {self.index.m}, ef_construction: {self.index.ef_construction} \"\n",
    "                        f\"and distance metric: {index_distance}\"\n",
    "                    )\n",
    "                    sess.execute(\n",
    "                        text(\n",
    "                            f\"CREATE INDEX IF NOT EXISTS {self.index.name} ON {self.table} \"\n",
    "                            f\"USING hnsw (embedding {index_distance}) \"\n",
    "                            f\"WITH (m = {self.index.m}, ef_construction = {self.index.ef_construction});\"\n",
    "                        )\n",
    "                    )\n",
    "        logger.debug(\"==== Optimized Vector DB ====\")\n",
    "\n",
    "    def delete(self) -> bool:\n",
    "        from sqlalchemy import delete\n",
    "\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = delete(self.table)\n",
    "                sess.execute(stmt)\n",
    "                return True\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"\n",
    "        Create a deep copy of the PgVector instance, handling unpickleable attributes.\n",
    "\n",
    "        Args:\n",
    "            memo (dict): A dictionary of objects already copied during the current copying pass.\n",
    "\n",
    "        Returns:\n",
    "            PgVector: A deep-copied instance of PgVector.\n",
    "        \"\"\"\n",
    "        from copy import deepcopy\n",
    "\n",
    "        # Create a new instance without calling __init__\n",
    "        cls = self.__class__\n",
    "        copied_obj = cls.__new__(cls)\n",
    "        memo[id(self)] = copied_obj\n",
    "\n",
    "        # Deep copy attributes\n",
    "        for k, v in self.__dict__.items():\n",
    "            if k in {\"metadata\", \"table\"}:\n",
    "                continue\n",
    "            # Reuse db_engine and Session without copying\n",
    "            elif k in {\"db_engine\", \"Session\", \"embedder\"}:\n",
    "                setattr(copied_obj, k, v)\n",
    "            else:\n",
    "                setattr(copied_obj, k, deepcopy(v, memo))\n",
    "\n",
    "        # Recreate metadata and table for the copied instance\n",
    "        copied_obj.metadata = MetaData(schema=copied_obj.schema)\n",
    "        copied_obj.table = copied_obj.get_table()\n",
    "\n",
    "        return copied_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from phi.embedder import Embedder\n",
    "\n",
    "\n",
    "class Document(BaseModel):\n",
    "    \"\"\"Model for managing a document\"\"\"\n",
    "\n",
    "    content: str\n",
    "    id: Optional[str] = None\n",
    "    name: Optional[str] = None\n",
    "    meta_data: Dict[str, Any] = {}\n",
    "    embedder: Optional[Embedder] = None\n",
    "    embedding: Optional[List[float]] = None\n",
    "    usage: Optional[Dict[str, Any]] = None\n",
    "    reranking_score: Optional[float] = None\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def embed(self, embedder: Optional[Embedder] = None) -> None:\n",
    "        \"\"\"Embed the document using the provided embedder\"\"\"\n",
    "\n",
    "        _embedder = embedder or self.embedder\n",
    "        if _embedder is None:\n",
    "            raise ValueError(\"No embedder provided\")\n",
    "\n",
    "        self.embedding, self.usage = _embedder.get_embedding_and_usage(self.content)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Returns a dictionary representation of the document\"\"\"\n",
    "\n",
    "        return self.model_dump(include={\"name\", \"meta_data\", \"content\"}, exclude_none=True)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, document: Dict[str, Any]) -> \"Document\":\n",
    "        \"\"\"Returns a Document object from a dictionary representation\"\"\"\n",
    "\n",
    "        return cls.model_validate(**document)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, document: str) -> \"Document\":\n",
    "        \"\"\"Returns a Document object from a json string representation\"\"\"\n",
    "\n",
    "        return cls.model_validate_json(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from phi.embedder import Embedder\n",
    "\n",
    "\n",
    "class Document(BaseModel):\n",
    "    \"\"\"Model for managing a document\"\"\"\n",
    "\n",
    "    content: str\n",
    "    id: Optional[str] = None\n",
    "    name: Optional[str] = None\n",
    "    meta_data: Dict[str, Any] = {}\n",
    "    embedder: Optional[Embedder] = None\n",
    "    embedding: Optional[List[float]] = None\n",
    "    usage: Optional[Dict[str, Any]] = None\n",
    "    reranking_score: Optional[float] = None\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def embed(self, embedder: Optional[Embedder] = None) -> None:\n",
    "        \"\"\"Embed the document using the provided embedder\"\"\"\n",
    "\n",
    "        _embedder = embedder or self.embedder\n",
    "        if _embedder is None:\n",
    "            raise ValueError(\"No embedder provided\")\n",
    "\n",
    "        # Modify this part to handle GroqEmbedder (which might only return an embedding)\n",
    "        # Check if embedder provides both embedding and usage or just embedding\n",
    "        try:\n",
    "            # Trying to get both embedding and usage, assuming `get_embedding_and_usage` exists\n",
    "            self.embedding, self.usage = _embedder.get_embedding_and_usage(self.content)\n",
    "        except AttributeError:\n",
    "            # If the embedder doesn't provide `get_embedding_and_usage`, only get the embedding\n",
    "            self.embedding = _embedder.get_embedding(self.content)  # Assuming this method exists\n",
    "            self.usage = None  # Set usage to None if not available\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Returns a dictionary representation of the document\"\"\"\n",
    "\n",
    "        return self.model_dump(include={\"name\", \"meta_data\", \"content\"}, exclude_none=True)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, document: Dict[str, Any]) -> \"Document\":\n",
    "        \"\"\"Returns a Document object from a dictionary representation\"\"\"\n",
    "\n",
    "        return cls.model_validate(**document)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, document: str) -> \"Document\":\n",
    "        \"\"\"Returns a Document object from a json string representation\"\"\"\n",
    "\n",
    "        return cls.model_validate_json(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Union, Dict, Any\n",
    "from hashlib import md5\n",
    "\n",
    "try:\n",
    "    from sqlalchemy.dialects import postgresql\n",
    "    from sqlalchemy.engine import create_engine, Engine\n",
    "    from sqlalchemy.inspection import inspect\n",
    "    from sqlalchemy.orm import Session, sessionmaker\n",
    "    from sqlalchemy.schema import MetaData, Table, Column\n",
    "    from sqlalchemy.sql.expression import text, func, select\n",
    "    from sqlalchemy.types import DateTime, String\n",
    "except ImportError:\n",
    "    raise ImportError(\"`sqlalchemy` not installed\")\n",
    "\n",
    "try:\n",
    "    from pgvector.sqlalchemy import Vector\n",
    "except ImportError:\n",
    "    raise ImportError(\"`pgvector` not installed\")\n",
    "\n",
    "from phi.document import Document\n",
    "from phi.embedder import Embedder\n",
    "from phi.vectordb.base import VectorDb\n",
    "from phi.vectordb.distance import Distance\n",
    "from phi.vectordb.pgvector.index import Ivfflat, HNSW\n",
    "from phi.utils.log import logger\n",
    "from phi.reranker.base import Reranker\n",
    "\n",
    "class PgVector2(VectorDb):\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection: str,\n",
    "        schema: Optional[str] = \"ai\",\n",
    "        db_url: Optional[str] = None,\n",
    "        db_engine: Optional[Engine] = None,\n",
    "        embedder: Optional[Embedder] = None,\n",
    "        distance: Distance = Distance.cosine,\n",
    "        index: Optional[Union[Ivfflat, HNSW]] = HNSW(),\n",
    "        reranker: Optional[Reranker] = None,\n",
    "        embedding_dimensions: Optional[int] = None  # New parameter for dimensions\n",
    "    ):\n",
    "        _engine: Optional[Engine] = db_engine\n",
    "        if _engine is None and db_url is not None:\n",
    "            _engine = create_engine(db_url)\n",
    "\n",
    "        if _engine is None:\n",
    "            raise ValueError(\"Must provide either db_url or db_engine\")\n",
    "\n",
    "        # Collection attributes\n",
    "        self.collection: str = collection\n",
    "        self.schema: Optional[str] = schema\n",
    "\n",
    "        # Database attributes\n",
    "        self.db_url: Optional[str] = db_url\n",
    "        self.db_engine: Engine = _engine\n",
    "        self.metadata: MetaData = MetaData(schema=self.schema)\n",
    "\n",
    "        # Embedder for embedding the document contents\n",
    "        _embedder = embedder\n",
    "        if _embedder is None:\n",
    "            # Import GroqEmbedder instead of OpenAIEmbedder\n",
    "            from groq_embedder import GroqEmbedder  # Import your custom GroqEmbedder\n",
    "            _embedder = GroqEmbedder(api_key=\"your_groq_api_key\")  # Initialize GroqEmbedder with your API key\n",
    "\n",
    "        self.embedder: Embedder = _embedder\n",
    "\n",
    "        # If GroqEmbedder does not have a `dimensions` attribute, use `embedding_dimensions` directly\n",
    "        if embedding_dimensions is None:\n",
    "            # Example: You may need to fetch this info from GroqEmbedder if possible\n",
    "            # For example, if GroqEmbedder has a `get_dimensions()` method, replace the line below:\n",
    "            self.dimensions = getattr(self.embedder, 'dimensions', embedding_dimensions)\n",
    "        else:\n",
    "            self.dimensions = embedding_dimensions\n",
    "\n",
    "        # Distance metric\n",
    "        self.distance: Distance = distance\n",
    "\n",
    "        # Reranker instance\n",
    "        self.reranker: Optional[Reranker] = reranker\n",
    "\n",
    "        # Index for the collection\n",
    "        self.index: Optional[Union[Ivfflat, HNSW]] = index\n",
    "\n",
    "        # Database session\n",
    "        self.Session: sessionmaker[Session] = sessionmaker(bind=self.db_engine)\n",
    "\n",
    "        # Database table for the collection\n",
    "        self.table: Table = self.get_table()\n",
    "\n",
    "    def get_table(self) -> Table:\n",
    "        return Table(\n",
    "            self.collection,\n",
    "            self.metadata,\n",
    "            Column(\"id\", String, primary_key=True),\n",
    "            Column(\"name\", String),\n",
    "            Column(\"meta_data\", postgresql.JSONB, server_default=text(\"'{}'::jsonb\")),\n",
    "            Column(\"content\", postgresql.TEXT),\n",
    "            Column(\"embedding\", Vector(self.dimensions)),\n",
    "            Column(\"usage\", postgresql.JSONB),\n",
    "            Column(\"created_at\", DateTime(timezone=True), server_default=text(\"now()\")),\n",
    "            Column(\"updated_at\", DateTime(timezone=True), onupdate=text(\"now()\")),\n",
    "            Column(\"content_hash\", String),\n",
    "            extend_existing=True,\n",
    "        )\n",
    "\n",
    "    def table_exists(self) -> bool:\n",
    "        logger.debug(f\"Checking if table exists: {self.table.name}\")\n",
    "        try:\n",
    "            return inspect(self.db_engine).has_table(self.table.name, schema=self.schema)\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            return False\n",
    "\n",
    "    def create(self) -> None:\n",
    "        if not self.table_exists():\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(\"Creating extension: vector\")\n",
    "                    sess.execute(text(\"create extension if not exists vector;\"))\n",
    "                    if self.schema is not None:\n",
    "                        logger.debug(f\"Creating schema: {self.schema}\")\n",
    "                        sess.execute(text(f\"create schema if not exists {self.schema};\"))\n",
    "            logger.debug(f\"Creating table: {self.collection}\")\n",
    "            self.table.create(self.db_engine)\n",
    "\n",
    "    def doc_exists(self, document: Document) -> bool:\n",
    "        \"\"\"\n",
    "        Validating if the document exists or not\n",
    "\n",
    "        Args:\n",
    "            document (Document): Document to validate\n",
    "        \"\"\"\n",
    "        columns = [self.table.c.name, self.table.c.content_hash]\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                stmt = select(*columns).where(self.table.c.content_hash == md5(cleaned_content.encode()).hexdigest())\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def name_exists(self, name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if a row with this name exists or not\n",
    "\n",
    "        Args:\n",
    "            name (str): Name to check\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(self.table.c.name).where(self.table.c.name == name)\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def id_exists(self, id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate if a row with this id exists or not\n",
    "\n",
    "        Args:\n",
    "            id (str): Id to check\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(self.table.c.id).where(self.table.c.id == id)\n",
    "                result = sess.execute(stmt).first()\n",
    "                return result is not None\n",
    "\n",
    "    def insert(self, documents: List[Document], filters: Optional[Dict[str, Any]] = None, batch_size: int = 10) -> None:\n",
    "        with self.Session() as sess:\n",
    "            counter = 0\n",
    "            for document in documents:\n",
    "                document.embed(embedder=self.embedder)\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                content_hash = md5(cleaned_content.encode()).hexdigest()\n",
    "                _id = document.id or content_hash\n",
    "                stmt = postgresql.insert(self.table).values(\n",
    "                    id=_id,\n",
    "                    name=document.name,\n",
    "                    meta_data=document.meta_data,\n",
    "                    content=cleaned_content,\n",
    "                    embedding=document.embedding,\n",
    "                    usage=document.usage,\n",
    "                    content_hash=content_hash,\n",
    "                )\n",
    "                sess.execute(stmt)\n",
    "                counter += 1\n",
    "                logger.debug(f\"Inserted document: {document.name} ({document.meta_data})\")\n",
    "\n",
    "                # Commit every `batch_size` documents\n",
    "                if counter >= batch_size:\n",
    "                    sess.commit()\n",
    "                    logger.info(f\"Committed {counter} documents\")\n",
    "                    counter = 0\n",
    "\n",
    "            # Commit any remaining documents\n",
    "            if counter > 0:\n",
    "                sess.commit()\n",
    "                logger.info(f\"Committed {counter} documents\")\n",
    "\n",
    "    def upsert_available(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def upsert(self, documents: List[Document], filters: Optional[Dict[str, Any]] = None, batch_size: int = 20) -> None:\n",
    "        \"\"\"\n",
    "        Upsert documents into the database.\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of documents to upsert\n",
    "            filters (Optional[Dict[str, Any]]): Filters to apply while upserting documents\n",
    "            batch_size (int): Batch size for upserting documents\n",
    "        \"\"\"\n",
    "        with self.Session() as sess:\n",
    "            counter = 0\n",
    "            for document in documents:\n",
    "                document.embed(embedder=self.embedder)\n",
    "                cleaned_content = document.content.replace(\"\\x00\", \"\\ufffd\")\n",
    "                content_hash = md5(cleaned_content.encode()).hexdigest()\n",
    "                _id = document.id or content_hash\n",
    "                stmt = postgresql.insert(self.table).values(\n",
    "                    id=_id,\n",
    "                    name=document.name,\n",
    "                    meta_data=document.meta_data,\n",
    "                    content=cleaned_content,\n",
    "                    embedding=document.embedding,\n",
    "                    usage=document.usage,\n",
    "                    content_hash=content_hash,\n",
    "                )\n",
    "                # Update row when id matches but 'content_hash' is different\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=[\"id\"],\n",
    "                    set_=dict(\n",
    "                        name=stmt.excluded.name,\n",
    "                        meta_data=stmt.excluded.meta_data,\n",
    "                        content=stmt.excluded.content,\n",
    "                        embedding=stmt.excluded.embedding,\n",
    "                        usage=stmt.excluded.usage,\n",
    "                        content_hash=stmt.excluded.content_hash,\n",
    "                        updated_at=text(\"now()\"),\n",
    "                    ),\n",
    "                )\n",
    "                sess.execute(stmt)\n",
    "                counter += 1\n",
    "                logger.debug(f\"Upserted document: {document.id} | {document.name} | {document.meta_data}\")\n",
    "\n",
    "                # Commit every `batch_size` documents\n",
    "                if counter >= batch_size:\n",
    "                    sess.commit()\n",
    "                    logger.info(f\"Committed {counter} documents\")\n",
    "                    counter = 0\n",
    "\n",
    "            # Commit any remaining documents\n",
    "            if counter > 0:\n",
    "                sess.commit()\n",
    "                logger.info(f\"Committed {counter} documents\")\n",
    "\n",
    "    def search(self, query: str, limit: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Document]:\n",
    "        query_embedding = self.embedder.get_embedding(query)\n",
    "        if query_embedding is None:\n",
    "            logger.error(f\"Error getting embedding for Query: {query}\")\n",
    "            return []\n",
    "\n",
    "        columns = [\n",
    "            self.table.c.name,\n",
    "            self.table.c.meta_data,\n",
    "            self.table.c.content,\n",
    "            self.table.c.embedding,\n",
    "            self.table.c.usage,\n",
    "        ]\n",
    "\n",
    "        stmt = select(*columns)\n",
    "\n",
    "        if filters is not None:\n",
    "            for key, value in filters.items():\n",
    "                if hasattr(self.table.c, key):\n",
    "                    stmt = stmt.where(getattr(self.table.c, key) == value)\n",
    "\n",
    "        if self.distance == Distance.l2:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.max_inner_product(query_embedding))\n",
    "        if self.distance == Distance.cosine:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.cosine_distance(query_embedding))\n",
    "        if self.distance == Distance.max_inner_product:\n",
    "            stmt = stmt.order_by(self.table.c.embedding.max_inner_product(query_embedding))\n",
    "\n",
    "        stmt = stmt.limit(limit=limit)\n",
    "        logger.debug(f\"Query: {stmt}\")\n",
    "\n",
    "        # Get neighbors\n",
    "        try:\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    if self.index is not None:\n",
    "                        if isinstance(self.index, Ivfflat):\n",
    "                            sess.execute(text(f\"SET LOCAL ivfflat.probes = {self.index.probes}\"))\n",
    "                        elif isinstance(self.index, HNSW):\n",
    "                            sess.execute(text(f\"SET LOCAL hnsw.ef_search  = {self.index.ef_search}\"))\n",
    "                    neighbors = sess.execute(stmt).fetchall() or []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching for documents: {e}\")\n",
    "            logger.error(\"Table might not exist, creating for future use\")\n",
    "            self.create()\n",
    "            return []\n",
    "\n",
    "        # Build search results\n",
    "        search_results: List[Document] = []\n",
    "        for neighbor in neighbors:\n",
    "            search_results.append(\n",
    "                Document(\n",
    "                    name=neighbor.name,\n",
    "                    meta_data=neighbor.meta_data,\n",
    "                    content=neighbor.content,\n",
    "                    embedder=self.embedder,\n",
    "                    embedding=neighbor.embedding,\n",
    "                    usage=neighbor.usage,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if self.reranker:\n",
    "            search_results = self.reranker.rerank(query=query, documents=search_results)\n",
    "\n",
    "        return search_results\n",
    "\n",
    "    def drop(self) -> None:\n",
    "        if self.table_exists():\n",
    "            logger.debug(f\"Deleting table: {self.collection}\")\n",
    "            self.table.drop(self.db_engine)\n",
    "\n",
    "    def exists(self) -> bool:\n",
    "        return self.table_exists()\n",
    "\n",
    "    def get_count(self) -> int:\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = select(func.count(self.table.c.name)).select_from(self.table)\n",
    "                result = sess.execute(stmt).scalar()\n",
    "                if result is not None:\n",
    "                    return int(result)\n",
    "                return 0\n",
    "\n",
    "    def optimize(self) -> None:\n",
    "        from math import sqrt\n",
    "\n",
    "        logger.debug(\"==== Optimizing Vector DB ====\")\n",
    "        if self.index is None:\n",
    "            return\n",
    "\n",
    "        if self.index.name is None:\n",
    "            _type = \"ivfflat\" if isinstance(self.index, Ivfflat) else \"hnsw\"\n",
    "            self.index.name = f\"{self.collection}_{_type}_index\"\n",
    "\n",
    "        index_distance = \"vector_cosine_ops\"\n",
    "        if self.distance == Distance.l2:\n",
    "            index_distance = \"vector_l2_ops\"\n",
    "        if self.distance == Distance.max_inner_product:\n",
    "            index_distance = \"vector_ip_ops\"\n",
    "\n",
    "        if isinstance(self.index, Ivfflat):\n",
    "            num_lists = self.index.lists\n",
    "            if self.index.dynamic_lists:\n",
    "                total_records = self.get_count()\n",
    "                logger.debug(f\"Number of records: {total_records}\")\n",
    "                if total_records < 1000000:\n",
    "                    num_lists = int(total_records / 1000)\n",
    "                elif total_records > 1000000:\n",
    "                    num_lists = int(sqrt(total_records))\n",
    "\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(f\"Setting configuration: {self.index.configuration}\")\n",
    "                    for key, value in self.index.configuration.items():\n",
    "                        sess.execute(text(f\"SET {key} = '{value}';\"))\n",
    "                    logger.debug(\n",
    "                        f\"Creating Ivfflat index with lists: {num_lists}, probes: {self.index.probes} \"\n",
    "                        f\"and distance metric: {index_distance}\"\n",
    "                    )\n",
    "                    sess.execute(text(f\"SET ivfflat.probes = {self.index.probes};\"))\n",
    "                    sess.execute(\n",
    "                        text(\n",
    "                            f\"CREATE INDEX IF NOT EXISTS {self.index.name} ON {self.table} \"\n",
    "                            f\"USING ivfflat (embedding {index_distance}) \"\n",
    "                            f\"WITH (lists = {num_lists});\"\n",
    "                        )\n",
    "                    )\n",
    "        elif isinstance(self.index, HNSW):\n",
    "            with self.Session() as sess:\n",
    "                with sess.begin():\n",
    "                    logger.debug(f\"Setting configuration: {self.index.configuration}\")\n",
    "                    for key, value in self.index.configuration.items():\n",
    "                        sess.execute(text(f\"SET {key} = '{value}';\"))\n",
    "                    logger.debug(\n",
    "                        f\"Creating HNSW index with m: {self.index.m}, ef_construction: {self.index.ef_construction} \"\n",
    "                        f\"and distance metric: {index_distance}\"\n",
    "                    )\n",
    "                    sess.execute(\n",
    "                        text(\n",
    "                            f\"CREATE INDEX IF NOT EXISTS {self.index.name} ON {self.table} \"\n",
    "                            f\"USING hnsw (embedding {index_distance}) \"\n",
    "                            f\"WITH (m = {self.index.m}, ef_construction = {self.index.ef_construction});\"\n",
    "                        )\n",
    "                    )\n",
    "        logger.debug(\"==== Optimized Vector DB ====\")\n",
    "\n",
    "    def delete(self) -> bool:\n",
    "        from sqlalchemy import delete\n",
    "\n",
    "        with self.Session() as sess:\n",
    "            with sess.begin():\n",
    "                stmt = delete(self.table)\n",
    "                sess.execute(stmt)\n",
    "                return True\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"\n",
    "        Create a deep copy of the PgVector instance, handling unpickleable attributes.\n",
    "\n",
    "        Args:\n",
    "            memo (dict): A dictionary of objects already copied during the current copying pass.\n",
    "\n",
    "        Returns:\n",
    "            PgVector: A deep-copied instance of PgVector.\n",
    "        \"\"\"\n",
    "        from copy import deepcopy\n",
    "\n",
    "        # Create a new instance without calling __init__\n",
    "        cls = self.__class__\n",
    "        copied_obj = cls.__new__(cls)\n",
    "        memo[id(self)] = copied_obj\n",
    "\n",
    "        # Deep copy attributes\n",
    "        for k, v in self.__dict__.items():\n",
    "            if k in {\"metadata\", \"table\"}:\n",
    "                continue\n",
    "            # Reuse db_engine and Session without copying\n",
    "            elif k in {\"db_engine\", \"Session\", \"embedder\"}:\n",
    "                setattr(copied_obj, k, v)\n",
    "            else:\n",
    "                setattr(copied_obj, k, deepcopy(v, memo))\n",
    "\n",
    "        # Recreate metadata and table for the copied instance\n",
    "        copied_obj.metadata = MetaData(schema=copied_obj.schema)\n",
    "        copied_obj.table = copied_obj.get_table()\n",
    "\n",
    "        return copied_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typer\n",
    "from typing import Optional,List\n",
    "from phi.assistant import Assistant\n",
    "from phi.storage.assistant.postgres import PgAssistantStorage\n",
    "\n",
    "from phi.knowledge.pdf import PDFUrlKnowledgeBase\n",
    "\n",
    "from phi.vectordb.pgvector import PgVector2\n",
    "\n",
    "from groq_embedder import GroqEmbedder\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ.pop(\"OPENAI_API_KEY\", None)  # to disable openai\n",
    "load_dotenv(dotenv_path='/Users/madmax_jos/Documents/agentaicourse/.env') # specify the full path if it's not recoginzed- dotenv_path='fullpath/.env'\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))  # if you think it's not loaded\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "db_url = \"postgresql+psycopg://ai:ai@localhost:5532/ai\"\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "# add this line only if you want to bypass embedding from openai i.e. phi.embedder.openai\n",
    "groq_embedder = GroqEmbedder(api_key=groq_api_key)\n",
    "\n",
    "knowledge_base=PDFUrlKnowledgeBase(\n",
    "    urls=[\"https://wwwcdn.imo.org/localresources/en/OurWork/Security/Documents/MSC.4-Circ.268_Annual%202023%20(1).pdf\"],\n",
    "    vector_db=PgVector2( collection=\"imo_marineattacksreport\",db_url=db_url, embedding_dimensions=512\n",
    "    ), embedder=groq_embedder) # To bypass openai embedder  - pass custom embedder parameter\n",
    "\n",
    "knowledge_base.load()\n",
    "\n",
    "storage=PgAssistantStorage(tablename=\"IMO_Marine_Attacks_pdf_assistant\", db_url=db_url)\n",
    "\n",
    "def IMO_Marine_Attacks_pdf_assistant(new: bool = False, user: str =\"user\"):\n",
    "    run_id: Optional[str] = None\n",
    "\n",
    "    if not new:\n",
    "        existing_runids: List[str]= storage.get_all_run_ids(user)\n",
    "        if len(existing_runids) >0:\n",
    "           run_id=existing_runids[0]\n",
    "\n",
    "    assis=Assistant( run_id=run_id, \n",
    "                        user_id=user,\n",
    "                        knowledge_base=knowledge_base,\n",
    "                        storage=storage,\n",
    "                        # display tool calls in response\n",
    "                        show_tool_calls=True,\n",
    "                        # To let the assistant to see the chat history- enable true\n",
    "                        read_chat_history= True, \n",
    "                        # Enable for assistant to parse through Knowledge base\n",
    "                        search_knowledge=True,)\n",
    "    \n",
    "    if run_id is None:\n",
    "        run_id=assis.run_id\n",
    "        print(f\"Started Run:  {run_id} \\n\")\n",
    "    else:\n",
    "        print(f\"ongoing Run:  {run_id} \\n\")\n",
    "    assis.cli_app(markdown=True)\n",
    "\n",
    "    if __name__==\"__main__\":\n",
    "        typer.run(IMO_Marine_Attacks_pdf_assistant)\n",
    "              \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
